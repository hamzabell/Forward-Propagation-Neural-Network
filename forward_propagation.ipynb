{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'd have to import the numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a single layer of our neural network and its input and output\n",
    "\n",
    "class FullyConnectedLayer(object):\n",
    "    def __init__(self, n_input, n_neurons):\n",
    "        # Randomly initialized the weights from the normal distribution\n",
    "        self.weights = np.random.randn(n_input, n_neurons)\n",
    "\n",
    "        # Define the bias for every neuron in this layer\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        "
   ]
  },
  {
   "source": [
    "# Each neuron perform the operation output = activation(x1 * w1 + x2 *w2 + ... + xn*wn+ bias)\n",
    "# first lets define the activation function, we'd use ReLU for the hidden layer and softmax for the \n",
    "# output which can put in context the output of the neuron net final layers.\n",
    "\n",
    "class ActivationFunction(object):\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "\n",
    "        return exp_values / np.mean(exp_values, axis=1, keepdims=True)\n",
    "        "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "source": [
    "# Now we can go ahead and define our dense neural network\n",
    "class NeuralNet(ActivationFunction):\n",
    "    def __init__(self, n_inputs, n_neurons, n_ouputs):\n",
    "        self.fc1 = FullyConnectedLayer(n_inputs, n_neurons)\n",
    "        self.fc2 = FullyConnectedLayer(n_neurons, 10)\n",
    "        self.fc3 = FullyConnectedLayer(10, n_ouputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.ReLU(np.dot(x, self.fc1.weights) + self.fc1.biases)\n",
    "        output = self.ReLU(np.dot(output, self.fc2.weights) + self.fc2.biases)\n",
    "        return self.softmax(np.dot(output, self.fc3.weights) + self.fc3.biases)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNet(4, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "    def negativelosslikelihood(self):\n",
    "        y_true = self.y_true.copy()\n",
    "        y_pred = self.y_pred.copy()\n",
    "\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "\n",
    "        if len(y_true.shape) == 1: \n",
    "            confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis= 1\n",
    "            )\n",
    "\n",
    "        return -np.log(confidences)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        output = self.negativelosslikelihood()\n",
    "\n",
    "        return np.mean(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lssfn = LossFunction(y, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.5957496"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "lssfn.calculate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}